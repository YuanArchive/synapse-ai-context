"""
ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €: í† í°ì„ ì ˆì•½í•˜ë©´ì„œ ì •ë³´ ì†ì‹¤ ì—†ì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
- **í† í° ì ˆì•½ â‰  ì •ë³´ ëˆ„ë½**: í† í° ì˜ˆì‚°ìœ¼ë¡œ ìë¥´ì§€ ì•Šê³ , Skeleton ë³€í™˜ìœ¼ë¡œ íš¨ìœ¨í™”
- Active íŒŒì¼: í•­ìƒ Full Code (ì ˆëŒ€ ìë¥´ì§€ ì•ŠìŒ)
- Reference íŒŒì¼: Skeletonìœ¼ë¡œ ì••ì¶• (êµ¬ì¡° ë³´ì¡´, êµ¬í˜„ ìƒì„¸ ì œê±°)
- Global ì»¨í…ìŠ¤íŠ¸: í”„ë¡œì íŠ¸ êµ¬ì¡° + ë©”íƒ€ë°ì´í„°

3-Level ê³„ì¸µ êµ¬ì¡°:
- Level 0 (Global): í”„ë¡œì íŠ¸ íŠ¸ë¦¬ + INTELLIGENCE.md
- Level 1 (Reference): Importëœ íŒŒì¼ë“¤ì˜ Skeleton
- Level 2 (Active): í˜„ì¬ ì‘ì—… íŒŒì¼ Full Code
"""

from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass, field
from collections import deque

from .structure.pruner import ASTSkeletonizer, SkeletonResult
from .graph import CodeGraph


@dataclass
class TokenSavings:
    """í† í° ì ˆì•½ íš¨ê³¼ ì¸¡ì •"""
    original_tokens: int
    optimized_tokens: int
    
    @property
    def saved_tokens(self) -> int:
        return self.original_tokens - self.optimized_tokens
    
    @property
    def savings_ratio(self) -> float:
        if self.original_tokens == 0:
            return 0.0
        return self.saved_tokens / self.original_tokens


@dataclass
class ContextResult:
    """ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ê²°ê³¼"""
    global_context: str       # Level 0
    reference_context: str    # Level 1 (Skeletons)
    active_context: str       # Level 2 (Full Code)
    
    total_tokens: int
    token_breakdown: Dict[str, int] = field(default_factory=dict)
    included_files: List[str] = field(default_factory=list)
    
    # í† í° ì ˆì•½ íš¨ê³¼
    savings: Optional[TokenSavings] = None
    
    @property
    def formatted_output(self) -> str:
        """í¬ë§·ëœ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì¶œë ¥"""
        sections = []
        
        if self.global_context:
            sections.append(f"# ğŸ“‚ Project Context\n\n{self.global_context}")
        
        if self.reference_context:
            sections.append(f"# ğŸ“š Reference Files (Skeleton)\n\n{self.reference_context}")
        
        if self.active_context:
            sections.append(f"# ğŸ¯ Active File (Full Code)\n\n{self.active_context}")
        
        return "\n\n---\n\n".join(sections)


class ContextManager:
    """
    3-Level ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì.
    
    **í•µì‹¬ ì›ì¹™**: í† í°ì„ "ì œí•œ"í•˜ì§€ ì•Šê³  "íš¨ìœ¨í™”"í•©ë‹ˆë‹¤.
    - Active íŒŒì¼ì€ í•­ìƒ Full Codeë¡œ í¬í•¨ (ì ˆëŒ€ ìë¥´ì§€ ì•ŠìŒ)
    - Reference íŒŒì¼ë“¤ì€ Skeletonìœ¼ë¡œ ì••ì¶•í•˜ì—¬ í† í° ì ˆì•½
    - í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°ë¡œ ì „ì²´ ë§¥ë½ ì œê³µ
    
    Example:
        >>> manager = ContextManager(Path("."))
        >>> result = manager.build_context(Path("src/main.py"))
        >>> print(f"í† í° ì ˆì•½: {result.savings.savings_ratio:.1%}")
        >>> print(result.formatted_output)
    """
    
    # í† í° ì¶”ì • ìƒìˆ˜ (chars per token)
    CHARS_PER_TOKEN = 4
    
    def __init__(
        self,
        project_path: Path,
        depth: int = 2,
        max_reference_files: int = 20
    ):
        """
        Args:
            project_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
            depth: ì˜ì¡´ì„± íƒìƒ‰ ê¹Šì´
            max_reference_files: ìµœëŒ€ Reference íŒŒì¼ ìˆ˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜)
        """
        self.project_path = Path(project_path).resolve()
        self.depth = depth
        self.max_reference_files = max_reference_files
        
        # .synapse ë””ë ‰í† ë¦¬ ê²½ë¡œ
        self.synapse_dir = self.project_path / ".synapse"
        
        # ì˜ì¡´ì„± ê·¸ë˜í”„ ë¡œë“œ
        self.graph = CodeGraph(storage_path=self.synapse_dir / "dependency_graph.gml")
        if (self.synapse_dir / "dependency_graph.gml").exists():
            self.graph.load()
        
        # Skeletonizer
        self.skeletonizer = ASTSkeletonizer()
    
    def build_context(self, active_file: Path) -> ContextResult:
        """
        Active íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        **ì •ë³´ ì†ì‹¤ ì—†ìŒ**: í† í° ì˜ˆì‚°ìœ¼ë¡œ ìë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ëŒ€ì‹  Reference íŒŒì¼ë“¤ì„ Skeletonìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í† í°ì„ ì ˆì•½í•©ë‹ˆë‹¤.
        
        Args:
            active_file: í˜„ì¬ ì‘ì—… ì¤‘ì¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ContextResult: ê³„ì¸µí™”ëœ ì»¨í…ìŠ¤íŠ¸ (ëª¨ë“  ì •ë³´ í¬í•¨)
        """
        active_file = Path(active_file).resolve()
        
        # ì›ë³¸ í† í° ì¶”ì ìš©
        original_tokens = 0
        
        # Level 0: Global Context (ì „ì²´ í¬í•¨)
        global_context, global_tokens = self._build_global_context()
        original_tokens += global_tokens
        
        # Level 2: Active Context (ì „ì²´ í¬í•¨, ìë¥´ì§€ ì•ŠìŒ)
        active_context, active_tokens, active_original = self._build_active_context(active_file)
        original_tokens += active_original
        
        # Level 1: Reference Context (Skeletonìœ¼ë¡œ ì••ì¶•)
        reference_context, reference_tokens, reference_original, included_files = \
            self._build_reference_context(active_file)
        original_tokens += reference_original
        
        total_tokens = global_tokens + reference_tokens + active_tokens
        
        # í† í° ì ˆì•½ íš¨ê³¼ ê³„ì‚°
        savings = TokenSavings(
            original_tokens=original_tokens,
            optimized_tokens=total_tokens
        )
        
        return ContextResult(
            global_context=global_context,
            reference_context=reference_context,
            active_context=active_context,
            total_tokens=total_tokens,
            token_breakdown={
                "global": global_tokens,
                "reference": reference_tokens,
                "active": active_tokens,
            },
            included_files=included_files,
            savings=savings
        )
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ë¬¸ì ìˆ˜ / 4 ê·¼ì‚¬ì¹˜)"""
        return len(text) // self.CHARS_PER_TOKEN
    
    def _build_global_context(self) -> Tuple[str, int]:
        """Level 0: í”„ë¡œì íŠ¸ ì „ì—­ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì „ì²´ í¬í•¨)"""
        parts = []
        
        # 1. INTELLIGENCE.md ì½ê¸°
        intel_path = self.synapse_dir / "INTELLIGENCE.md"
        if intel_path.exists():
            try:
                with open(intel_path, "r", encoding="utf-8") as f:
                    intel_content = f.read()
                parts.append(f"## Project Intelligence\n\n{intel_content}")
            except Exception:
                pass
        
        # 2. ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ íŠ¸ë¦¬ (ìµœëŒ€ 3ë ˆë²¨)
        tree_str = self._generate_tree(self.project_path, max_depth=3)
        if tree_str:
            parts.append(f"## Project Structure\n\n```\n{tree_str}\n```")
        
        global_context = "\n\n".join(parts)
        return global_context, self._estimate_tokens(global_context)
    
    def _build_active_context(self, active_file: Path) -> Tuple[str, int, int]:
        """
        Level 2: Active íŒŒì¼ ì „ì²´ ì½”ë“œ (ì ˆëŒ€ ìë¥´ì§€ ì•ŠìŒ)
        
        Returns:
            Tuple[str, int, int]: (ì»¨í…ìŠ¤íŠ¸, í† í°ìˆ˜, ì›ë³¸í† í°ìˆ˜)
        """
        if not active_file.exists():
            msg = f"# File not found: {active_file}"
            return msg, 10, 10
        
        try:
            with open(active_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            try:
                rel_path = active_file.relative_to(self.project_path)
            except ValueError:
                rel_path = active_file.name
            
            # ì–¸ì–´ ê°ì§€
            ext_to_lang = {
                ".py": "python", ".js": "javascript", ".ts": "typescript",
                ".jsx": "jsx", ".tsx": "tsx", ".go": "go", ".rs": "rust",
                ".java": "java", ".cpp": "cpp", ".c": "c"
            }
            lang = ext_to_lang.get(active_file.suffix.lower(), "")
            
            formatted = f"## {rel_path}\n\n```{lang}\n{content}\n```"
            tokens = self._estimate_tokens(formatted)
            
            # Active íŒŒì¼ì€ ì••ì¶•í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì›ë³¸ = ìµœì í™”
            return formatted, tokens, tokens
            
        except Exception as e:
            msg = f"# Error reading file: {e}"
            return msg, 10, 10
    
    def _build_reference_context(
        self, 
        active_file: Path
    ) -> Tuple[str, int, int, List[str]]:
        """
        Level 1: Importëœ íŒŒì¼ë“¤ì˜ Skeleton (ì •ë³´ ì†ì‹¤ ì—†ëŠ” ì••ì¶•)
        
        Returns:
            Tuple[str, int, int, List[str]]: 
                (ì»¨í…ìŠ¤íŠ¸, ìµœì í™”í† í°ìˆ˜, ì›ë³¸í† í°ìˆ˜, í¬í•¨íŒŒì¼ëª©ë¡)
        """
        # ê´€ë ¨ íŒŒì¼ ì°¾ê¸° (BFS)
        active_posix = active_file.as_posix()
        related_files = self._get_related_files_bfs(active_posix, self.depth)
        
        if not related_files:
            return "# No related files found", 5, 5, []
        
        # ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ì„±ëŠ¥)
        related_files = related_files[:self.max_reference_files]
        
        parts = []
        included = []
        optimized_tokens = 0
        original_tokens = 0
        
        for file_path in related_files:
            file_obj = Path(file_path)
            if not file_obj.exists():
                continue
            
            # ì›ë³¸ íŒŒì¼ ì½ê¸° (í† í° ì ˆì•½ ì¸¡ì •ìš©)
            try:
                with open(file_obj, "r", encoding="utf-8") as f:
                    original_content = f.read()
                original_tokens += self._estimate_tokens(original_content)
            except Exception:
                continue
            
            # Skeleton ìƒì„± (í† í° ì ˆì•½)
            result = self.skeletonizer.skeletonize_file(file_obj)
            
            try:
                rel_path = file_obj.relative_to(self.project_path)
            except ValueError:
                rel_path = file_obj.name
            
            # ì–¸ì–´ ê°ì§€
            ext_to_lang = {
                ".py": "python", ".js": "javascript", ".ts": "typescript",
                ".jsx": "jsx", ".tsx": "tsx", ".go": "go", ".rs": "rust",
                ".java": "java", ".cpp": "cpp", ".c": "c"
            }
            lang = ext_to_lang.get(file_obj.suffix.lower(), "")
            
            skeleton_text = f"### {rel_path}\n\n```{lang}\n{result.skeleton}\n```"
            skeleton_tokens = self._estimate_tokens(skeleton_text)
            
            parts.append(skeleton_text)
            optimized_tokens += skeleton_tokens
            included.append(str(rel_path))
        
        reference_context = "\n\n".join(parts) if parts else "# No reference files included"
        return reference_context, optimized_tokens, original_tokens, included
    
    def _get_related_files_bfs(self, file_path: str, depth: int) -> List[str]:
        """BFSë¡œ ê´€ë ¨ íŒŒì¼ íƒìƒ‰ (depth ê¸°ë°˜ ìš°ì„ ìˆœìœ„)"""
        if file_path not in self.graph.graph:
            return []
        
        visited = set()
        queue = deque([(file_path, 0)])
        result = []
        
        while queue:
            current, current_depth = queue.popleft()
            
            if current in visited or current_depth > depth:
                continue
            visited.add(current)
            
            # ì‹œì‘ íŒŒì¼ ìì²´ëŠ” ì œì™¸
            if current != file_path:
                # íŒŒì¼ ë…¸ë“œë§Œ ì¶”ê°€ (symbol ë…¸ë“œ ì œì™¸)
                node_data = self.graph.graph.nodes.get(current, {})
                if node_data.get("type") == "file":
                    result.append((current, current_depth))
            
            # ì¸ì ‘ ë…¸ë“œ íƒìƒ‰
            for neighbor in self.graph.graph.successors(current):
                if neighbor not in visited:
                    queue.append((neighbor, current_depth + 1))
            
            for neighbor in self.graph.graph.predecessors(current):
                if neighbor not in visited:
                    queue.append((neighbor, current_depth + 1))
        
        # depth ê¸°ì¤€ ì •ë ¬ (ê°€ê¹Œìš´ íŒŒì¼ ìš°ì„ )
        result.sort(key=lambda x: x[1])
        return [f for f, d in result]
    
    def _generate_tree(self, path: Path, max_depth: int = 3, prefix: str = "") -> str:
        """ê°„ë‹¨í•œ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ìƒì„±"""
        exclude_dirs = {
            ".git", ".venv", "venv", "__pycache__", "node_modules",
            ".synapse", ".idea", ".vscode", "dist", "build"
        }
        
        lines = []
        
        try:
            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
        except PermissionError:
            return ""
        
        # í•­ëª© í•„í„°ë§
        entries = [e for e in entries if e.name not in exclude_dirs]
        
        for i, entry in enumerate(entries[:20]):  # ìµœëŒ€ 20ê°œ í•­ëª©
            is_last = i == len(entries) - 1 or i == 19
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if entry.is_dir():
                lines.append(f"{prefix}{connector}{entry.name}/")
                if max_depth > 1:
                    extension = "    " if is_last else "â”‚   "
                    subtree = self._generate_tree(entry, max_depth - 1, prefix + extension)
                    if subtree:
                        lines.append(subtree)
            else:
                lines.append(f"{prefix}{connector}{entry.name}")
        
        if len(entries) > 20:
            lines.append(f"{prefix}â””â”€â”€ ... ({len(entries) - 20} more items)")
        
        return "\n".join(lines)


# í¸ì˜ í•¨ìˆ˜
def build_context(
    project_path: str,
    active_file: str,
    depth: int = 2
) -> ContextResult:
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹Œë“œí•˜ëŠ” í¸ì˜ í•¨ìˆ˜.
    
    í† í°ì„ "ì œí•œ"í•˜ì§€ ì•Šê³  "íš¨ìœ¨í™”"í•©ë‹ˆë‹¤.
    Reference íŒŒì¼ë“¤ì€ Skeletonìœ¼ë¡œ ì••ì¶•ë˜ì–´ í† í°ì´ ì ˆì•½ë©ë‹ˆë‹¤.
    
    Args:
        project_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        active_file: í˜„ì¬ ì‘ì—… íŒŒì¼ ê²½ë¡œ
        depth: ì˜ì¡´ì„± íƒìƒ‰ ê¹Šì´
        
    Returns:
        ContextResult: ë¹Œë“œëœ ì»¨í…ìŠ¤íŠ¸ (ì •ë³´ ì†ì‹¤ ì—†ìŒ)
    """
    manager = ContextManager(
        project_path=Path(project_path),
        depth=depth
    )
    return manager.build_context(Path(active_file))
